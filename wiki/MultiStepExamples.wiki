#summary Inference Examples

This file contains a group of multi-step inference examples, which shows the
expressive and inferential capability of NARS.

Each example starts at a line with a {{{*}}} to empty the memory of the system. 
To run an example, copy/paste that line and the following lines into the input 
window of the NARS applet. For instance, in the 
first example, it includes the first 6 lines. What is listed after the input 
lines are the lines displayed in the main window of NARS, followed by a brief 
explanation of the example.

----
===Choice===
Input:
{{{
*
<robin --> bird>.
<swan --> bird>. %1;0.8%
<penguin --> bird>. %0.8%
5
<?x --> bird>?
30
}}}
Display:
{{{
  IN: <robin --> bird>. %1.00;0.90% {1: 1} 
  IN: <swan --> bird>. %1.00;0.80% {1: 2} 
  IN: <penguin --> bird>. %0.80;0.90% {1: 3} 
5
  IN: <?1 --> bird>?  {1: 4} 
17
 OUT: <penguin --> bird>. %0.80;0.90% {1: 3} 
13
 OUT: <robin --> bird>. %1.00;0.90% {1: 1} 
}}}
When a question has more than one candidate answers, their order of evaluation 
is highly context-sensative. The system reports the best it has found so far, 
and therefore may report more than one answer to a given question. In this example,
the system will settle down at the last answer even if it is given longer time.

----
{{{
*** Contradiction ***
<coffee --> beverage>.
<Java --> coffee>.
(--,<Java --> coffee>).
10 
<Java --> coffee>?
<coffee --> beverage>?
1
}}}
{{{
  IN: <coffee --> beverage>. %1.00;0.90% {1: 1} 
  IN: <Java --> coffee>. %1.00;0.90% {1: 2} 
  IN: (--,<Java --> coffee>). %1.00;0.90% {1: 3} 
10
  IN: <Java --> coffee>?  {1: 4} 
  IN: <coffee --> beverage>?  {1: 5} 
1
 OUT: <Java --> coffee>. %0.50;0.94% {2: 3;2} 
 OUT: <coffee --> beverage>. %1.00;0.90% {1: 1} 
}}}
A contradiction makes the system unsure on directly related questions, but will
not make the system to derive an arbitrary conclusion on other questions, as in 
propositional logic.

----
{{{
*** Confidence and revision ***
<Willy {-- swimmer>.
<fish --> swimmer>.
<Willy {-- fish>?
10
<Willy {-- whale>.
<whale --] black>.
<Willy {-] black>?
20  
<Willy {-] black>. %0%
<Willy {-- fish>. %0%
10
}}}
{{{
  IN: <{Willy} --> swimmer>. %1.00;0.90% {1: 1} 
  IN: <fish --> swimmer>. %1.00;0.90% {1: 2} 
  IN: <{Willy} --> fish>?  {1: 3} 
2
 OUT: <{Willy} --> fish>. %1.00;0.44% {2: 1;2} 
8
  IN: <{Willy} --> whale>. %1.00;0.90% {1: 4} 
  IN: <whale --> [black]>. %1.00;0.90% {1: 5} 
  IN: <{Willy} --> [black]>?  {1: 6} 
17
 OUT: <{Willy} --> [black]>. %1.00;0.81% {2: 4;5} 
3
  IN: <{Willy} --> [black]>. %0.00;0.90% {1: 7} 
  IN: <{Willy} --> fish>. %0.00;0.90% {1: 8} 
1
 OUT: <{Willy} --> [black]>. %0.00;0.90% {1: 7} 
 OUT: <{Willy} --> fish>. %0.00;0.90% {1: 8} 
1
 OUT: <{Willy} --> [black]>. %0.32;0.92% {3: 4;7;5} 
 OUT: <{Willy} --> fish>. %0.08;0.90% {3: 1;8;2} 
}}}
Even when all the input judgments using the default confidence value, different rules
produce conclusions with difference confidence, which have different sensitivity when
facing the same amount of new evidence.

----
{{{
*** Deduction chain ***
<Tweety {-- robin>.
<robin --> bird>.
<bird --> animal>.
10 
<Tweety {-- bird>?
10
<Tweety {-- animal>?
10
{{{
}}}
  IN: <{Tweety} --> robin>. %1.00;0.90% {1: 1} 
  IN: <robin --> bird>. %1.00;0.90% {1: 2} 
  IN: <bird --> animal>. %1.00;0.90% {1: 3} 
10
  IN: <{Tweety} --> bird>?  {1: 4} 
1
 OUT: <{Tweety} --> bird>. %1.00;0.81% {2: 1;2} 
9
  IN: <{Tweety} --> animal>?  {1: 5} 
1
 OUT: <{Tweety} --> animal>. %1.00;0.72% {3: 1;3;2} 
}}}
The conclusion of a previous step may be used as a premise in a following step.
In the example, though both answers are positive (with frequency 1), their
confidence is getting lower as the deduction chain gets longer.

----
{{{
*** Resemblance Chain ***
<dog <-> cat>. %0.9%
<cat <-> tiger>. %0.9%
<tiger <-> lion>. %0.9%
<dog <-> lion>?
10
}}}
{{{
  IN: <cat <-> dog>. %0.90;0.90% {1: 1} 
  IN: <cat <-> tiger>. %0.90;0.90% {1: 2} 
  IN: <lion <-> tiger>. %0.90;0.90% {1: 3} 
  IN: <dog <-> lion>?  {1: 4} 
5
 OUT: <dog <-> lion>. %0.72;0.70% {3: 2;1;3}
}}}
Given incomplete similarity, both frequency and the confidence decrease 
alone an inference chain.

----
{{{
*** Induction and revision ***
<bird --> swimmer>?
<swimmer --> bird>? 
1
<swan --> bird>.
<swan --> swimmer>.
10 
<gull --> bird>.
<gull --> swimmer>.
10
<crow --> bird>.
(--, <crow --> swimmer>).
30
}}}
{{{
  IN: <bird --> swimmer>?  {1: 1} 
  IN: <swimmer --> bird>?  {1: 2} 
1
  IN: <swan --> bird>. %1.00;0.90% {1: 3} 
  IN: <swan --> swimmer>. %1.00;0.90% {1: 4} 
3
 OUT: <swimmer --> bird>. %1.00;0.44% {2: 3;4} 
 OUT: <bird --> swimmer>. %1.00;0.44% {2: 3;4} 
7
  IN: <gull --> bird>. %1.00;0.90% {1: 5} 
  IN: <gull --> swimmer>. %1.00;0.90% {1: 6} 
4
 OUT: <swimmer --> bird>. %1.00;0.55% {4: 5;3;6;4} 
 OUT: <swimmer --> bird>. %1.00;0.61% {4: 5;3;6;4} 
 OUT: <bird --> swimmer>. %1.00;0.55% {4: 5;3;6;4} 
 OUT: <bird --> swimmer>. %1.00;0.61% {4: 5;3;6;4} 
6
  IN: <crow --> bird>. %1.00;0.90% {1: 7} 
  IN: (--,<crow --> swimmer>). %1.00;0.90% {1: 8} 
21
 OUT: <bird --> swimmer>. %0.60;0.67% {6: 5;7;3;8;6;4} 
 OUT: <bird --> swimmer>. %0.66;0.70% {6: 5;7;3;8;6;4} 
}}}
(1) Question may still be remembered before available knowledge arrives, or after 
answers are reported;
(2) The system can change its mind when new evidence is taken into consideration;
(3) Positive evidence has the same effect on symmetric inductive conclusions, but
negative evidence does not.

----
{{{
*** Mixed Inference ***
<swan --> bird>.
<swan --> swimmer>.
10
<gull --> bird>.
<gull --> swimmer>.
20
<robin --] feathered>.
<bird --] feathered>.
50
<robin --> swimmer>?
400
}}}
{{{
  IN: <swan --> bird>. %1.00;0.90% {1: 1} 
  IN: <swan --> swimmer>. %1.00;0.90% {1: 2} 
10
  IN: <gull --> bird>. %1.00;0.90% {1: 3} 
  IN: <gull --> swimmer>. %1.00;0.90% {1: 4} 
20
  IN: <robin --> [feathered]>. %1.00;0.90% {1: 5} 
  IN: <bird --> [feathered]>. %1.00;0.90% {1: 6} 
50
  IN: <robin --> swimmer>?  {1: 7} 
1
 OUT: <robin --> swimmer>. %1.00;0.16% {4: 2;5;1;6} 
273
 OUT: <robin --> swimmer>. %1.00;0.21% {6: 3;5;1;6;4;2} 
81
 OUT: <robin --> swimmer>. %1.00;0.32% {6: 3;5;6;1;4;2} 
}}}
The final conclusion is produced using induction, abduction, deduction, and revision.
The selection of inference rule is data driven, not specified explicitly in the
input. There is no guarantee that all relevant evidence will be taken into consideration.

----
{{{
*** Compositionality ***
<light --> traffic_signal>. %0.1% 
<[red] --> traffic_signal>. %0.1% 
10
<(&, [red], light) --> traffic_signal>?
10 
<light_1 {-- (&, [red], light)>.
<light_1 {-- traffic_signal>.
10
<light_2 {-- (&, [red], light)>.
<light_2 {-- traffic_signal>.
20
}}}
{{{
  IN: <light --> traffic_signal>. %0.10;0.90% {1: 1} 
  IN: <[red] --> traffic_signal>. %0.10;0.90% {1: 2} 
10
  IN: <(&,[red],light) --> traffic_signal>?  {1: 3} 
3
 OUT: <(&,[red],light) --> traffic_signal>. %0.19;0.82% {2: 1;2} 
7
  IN: <{light_1} --> (&,[red],light)>. %1.00;0.90% {1: 4} 
  IN: <{light_1} --> traffic_signal>. %1.00;0.90% {1: 5} 
3
 OUT: <(&,[red],light) --> traffic_signal>. %0.30;0.84% {4: 5;1;4;2} 
7
  IN: <{light_2} --> (&,[red],light)>. %1.00;0.90% {1: 6} 
  IN: <{light_2} --> traffic_signal>. %1.00;0.90% {1: 7} 
7
 OUT: <(&,[red],light) --> traffic_signal>. %0.39;0.86% {6: 5;7;1;6;4;2} 
}}}
Initially, the meaning of compound term "(&,[red],light)" is determined by the
meaning of its components "red" and "light", but it will no longer be the case
when the system gets experience about the compound that cannot be reduced to its
components.

----
{{{
*** Fuzzy Concept ***
<John {-- boy>.
<John {-- (/, taller_than, {Tom}, _)>.
20
<Tom {-- (/, taller_than, _, boy)>?
10
<David {-- boy>.
(--, <David {-- (/, taller_than, {Tom}, _)>).
20
<Karl {-- boy>.
<Karl {-- (/, taller_than, {Tom}, _)>. 
100
}}}
{{{
  IN: <{John} --> boy>. %1.00;0.90% {1: 1} 
  IN: <{John} --> (/,taller_than,{Tom},_)>. %1.00;0.90% {1: 2} 
20
  IN: <{Tom} --> (/,taller_than,_,boy)>?  {1: 3} 
3
 OUT: <{Tom} --> (/,taller_than,_,boy)>. %1.00;0.44% {2: 1;2} 
7
  IN: <{David} --> boy>. %1.00;0.90% {1: 4} 
  IN: (--,<{David} --> (/,taller_than,{Tom},_)>). %1.00;0.90% {1: 5} 
20
  IN: <{Karl} --> boy>. %1.00;0.90% {1: 6} 
  IN: <{Karl} --> (/,taller_than,{Tom},_)>. %1.00;0.90% {1: 7} 
70
 OUT: <{Tom} --> (/,taller_than,_,boy)>. %0.66;0.70% {6: 6;4;1;5;7;2} 
}}}
John's degree of membership to fuzzy concept "tall boy" depends on the extent 
to which he is taller than the other boys, determined according to available
evidence.